{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask for beginners Cheat Sheets sample code\n",
    "\n",
    "(c) 2020 NVIDIA, Blazing SQL\n",
    "\n",
    "Distributed under Apache License 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import numpy as np\n",
    "import dask_cudf\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client\n",
    "from dask.delayed import delayed\n",
    "import dask.distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cudf.DataFrame(\n",
    "    [\n",
    "          (39, 6.88, np.datetime64('2020-10-08T12:12:01'), 'C', 'D', 'data'\n",
    "            , 'RAPIDS.ai is a suite of open-source libraries that allow you to run your end to end data science and analytics pipelines on GPUs.')\n",
    "        , (11, 4.21, None,                                 'A', 'D', 'cuDF'\n",
    "            , 'cuDF is a Python GPU DataFrame (built on the Apache Arrow columnar memory format)')\n",
    "        , (31, 4.71, np.datetime64('2020-10-10T09:26:43'), 'U', 'D', 'memory'\n",
    "            , 'cuDF allows for loading, joining, aggregating, filtering, and otherwise manipulating tabular data using a DataFrame style API.')\n",
    "        , (40, 0.93, np.datetime64('2020-10-11T17:10:00'), 'P', 'B', 'tabular'\n",
    "            , '''If your workflow is fast enough on a single GPU or your data comfortably fits in memory on \n",
    "                 a single GPU, you would want to use cuDF.''')\n",
    "        , (33, 9.26, np.datetime64('2020-10-15T10:58:02'), 'O', 'D', 'parallel'\n",
    "            , '''If you want to distribute your workflow across multiple GPUs or have more data than you can fit \n",
    "                 in memory on a single GPU you would want to use Dask-cuDF''')\n",
    "        , (42, 4.21, np.datetime64('2020-10-01T10:02:23'), 'U', 'C', 'GPUs'\n",
    "            , 'BlazingSQL provides a high-performance distributed SQL engine in Python')\n",
    "        , (36, 3.01, np.datetime64('2020-09-30T14:36:26'), 'T', 'D', None\n",
    "            , 'BlazingSQL is built on the RAPIDS GPU data science ecosystem')\n",
    "        , (38, 6.44, np.datetime64('2020-10-10T08:34:36'), 'X', 'B', 'csv'\n",
    "            , 'BlazingSQL lets you ETL raw data directly into GPU memory as a GPU DataFrame (GDF)')\n",
    "        , (17, 5.28, np.datetime64('2020-10-09T08:34:40'), 'P', 'D', 'dataframes'\n",
    "            , 'Dask is a flexible library for parallel computing in Python')\n",
    "        , (10, 8.28, np.datetime64('2020-10-03T03:31:21'), 'W', 'B', 'python'\n",
    "            , None)\n",
    "    ]\n",
    "    , columns = ['number', 'float_number', 'datetime', 'letter', 'category', 'word', 'string']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster and client setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCUDACluster(\n",
    "    n_workers=1\n",
    "    , threads_per_worker=1\n",
    "    , CUDA_VISIBLE_DEVICES=\"0\"\n",
    "    , rmm_managed_memory=True\n",
    "    , rmm_pool_size=\"20GB\"\n",
    ")\n",
    "\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dask_cudf.DataFrame.from_cudf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dask_cudf.from_cudf(df, npartitions=2)\n",
    "ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dask_cudf.from_cudf(df, chunksize=2)\n",
    "ddf.npartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dask_cudf.DataFrame.map_partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame(df):\n",
    "    df['num_inc'] = df['number'] + 10\n",
    "    \n",
    "    return df\n",
    "    \n",
    "ddf.map_partitions(process_frame).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply(a, b, mult):\n",
    "    for i, (aa, bb) in enumerate(zip(a, b)):\n",
    "        mult[i] = aa * bb\n",
    "\n",
    "def process_frame_mul(df):\n",
    "    df = df.apply_rows(\n",
    "        multiply\n",
    "        , incols = {'number': 'a', 'float_number': 'b'}\n",
    "        , outcols = {'mult': np.float64}\n",
    "        , kwargs = {}\n",
    "    )\n",
    "    \n",
    "    return df['mult']\n",
    "\n",
    "ddf.map_partitions(process_frame_mul).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide(a, div, b):\n",
    "    for i, aa in enumerate(a):\n",
    "        div[i] = aa / b\n",
    "\n",
    "def process_frame_div(df, col_a, val_divide):\n",
    "    df = df.apply_rows(\n",
    "        divide\n",
    "        , incols = {col_a: 'a'}\n",
    "        , outcols = {'div': np.float64}\n",
    "        , kwargs = {'b': val_divide}\n",
    "    )\n",
    "    \n",
    "    return df['div']\n",
    "\n",
    "ddf['div_number'] = ddf.map_partitions(process_frame_div, 'number', 10.0)\n",
    "ddf['div_float']  = ddf.map_partitions(process_frame_div, 'float_number', 5.0)\n",
    "\n",
    "ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf['div_number'] = ddf.map_partitions(lambda df: process_frame_div(df, 'number', 10.0))\n",
    "ddf['div_float']  = ddf.map_partitions(lambda df: process_frame_div(df, 'float_number', 5.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dask_cudf.compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### client.compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computation = client.compute(ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computation.result().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computation = client.compute(ddf, optimize_graph=True, workers='0')\n",
    "computation.result().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dask_cudf.persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dask.delayed.delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "\n",
    "def delayed_task(n):\n",
    "    df = cudf.DataFrame({'random': cp.random.rand(n)})\n",
    "    df['rand_scaled'] = df['random'] * 3\n",
    "    return df\n",
    "\n",
    "tasks = [delayed(delayed_task)(10) for _ in range(2)]\n",
    "computation = client.compute(tasks, optimize_graph=True)\n",
    "computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "\n",
    "@delayed\n",
    "def delayed_task(n):\n",
    "    df = cudf.DataFrame({'random': cp.random.rand(n)})\n",
    "    df['rand_scaled'] = df['random'] * 3\n",
    "    return df\n",
    "\n",
    "tasks = [delayed_task(10) for _ in range(2)]\n",
    "computation = client.compute(tasks, optimize_graph=True)\n",
    "computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cudf.concat([f.result() for f in computation]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dask_cudf.DataFrame.to_delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame_delayed(df):\n",
    "    return df['number'] + 10\n",
    "    \n",
    "ddf_delayed_add = dask_cudf.from_delayed([\n",
    "    process_frame_delayed(df) \n",
    "    for df \n",
    "    in ddf.to_delayed()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_delayed_add.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dask_cudf.DataFrame.from_delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame_delayed(df, divide):\n",
    "    added = df['number'] + 10\n",
    "    \n",
    "    return added / divide\n",
    "    \n",
    "ddf_delayed_div = dask_cudf.from_delayed([\n",
    "    process_frame_delayed(df, 10.0) \n",
    "    for df \n",
    "    in ddf.to_delayed()\n",
    "])\n",
    "\n",
    "ddf_delayed_div.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Futures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### client.persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.persist(ddf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### client.submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_computation(df):\n",
    "    return df['number'] + 10\n",
    "\n",
    "def second_computation(result):\n",
    "    return result / 10.0\n",
    "\n",
    "computation_1 = client.submit(first_computation, ddf)\n",
    "computation_2 = client.submit(second_computation, computation_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computation_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computation_2.result().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dask.distributed.wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computation = client.compute(tasks, optimize_graph=True)\n",
    "dask.distributed.wait(computation)\n",
    "\n",
    "### this object only gets created one all computations are finished\n",
    "results = dask_cudf.from_delayed(computation)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dask.distributed.as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computation = client.compute(tasks, optimize_graph=True)\n",
    "\n",
    "for part in dask.distributed.as_completed(computation):\n",
    "    print(part.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Future.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_computation(df):\n",
    "    return df['number'] + 10\n",
    "\n",
    "computation_1 = client.submit(first_computation, ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computation_1.result().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Future.done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(computation_1.done())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### client.gather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.gather(computation_1).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### client.scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = client.gather(computation_1).compute()\n",
    "distributed = client.scatter(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### client.cancel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computation_1.cancel()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
