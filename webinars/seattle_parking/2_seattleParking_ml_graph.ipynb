{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parking in Seattle\n",
    "\n",
    "Driving in Seattle is quickly becoming very similar to driving in cities like San Francisco, Silicon Valley or Los Angeles: more and more companies choose to settle or open their offices in Seattle so they can tap into the tech community that Seattle has to offer. With that, parking in Seattle is getting harder by day.\n",
    "\n",
    "Paid Parking Occupancy dataset provided by the City of Seattle Department of Transportation provides a view into around 300 million parking transactions annually from around 12 thousands parking spots on roughly 1,500 block faces. The dataset does not include any transaction for Sundays as there is no paid parking. Most of the parking spots have a 2-hour limit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the modules\n",
    "\n",
    "First, we'll load the necessary modules and instantiate the `BlazingContext()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import numpy as np\n",
    "import cudf\n",
    "import dask_cudf\n",
    "import cugraph\n",
    "import cuspatial\n",
    "import datetime as dttm\n",
    "from cuml.preprocessing import train_test_split\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client\n",
    "from blazingsql import BlazingContext\n",
    "from os import listdir\n",
    "\n",
    "bc = BlazingContext()\n",
    "\n",
    "_ = bc.s3(\n",
    "    'bsql'\n",
    "    , bucket_name = 'bsql'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the tables\n",
    "\n",
    "First, we read the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_partitions_cnt = 40\n",
    "transactions_path = 's3://bsql/data/seattle_parking/parking_MayJun2019.parquet/partition_idx={partition}/'\n",
    "transactions_parq = [transactions_path.format(partition=p) for p in range(transactions_partitions_cnt)]\n",
    "\n",
    "locations_parq = 's3://bsql/data/seattle_parking/parking_locations.parquet/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc.create_table('parking_transactions', transactions_parq)\n",
    "bc.create_table('parking_locations', locations_parq)\n",
    "\n",
    "parking_locations = bc.sql('''\n",
    "    SELECT * \n",
    "    FROM parking_locations \n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featurize parking transactions\n",
    "\n",
    "In order to build a regression model we need to featurize our dataset a bit. In this simple apporach our only variable is going to be the average occupancy at a parking spot at 1 hour prior to the transaction.\n",
    "\n",
    "But first, let's remove outliers: transactions that reported higher occupancy than available parking spots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_time_deltas(df, delta, time_signature):\n",
    "    return df['OccupancyDateTime'] + np.timedelta64(delta, time_signature)\n",
    "\n",
    "def mark_outliers(df):\n",
    "    return df['PaidOccupancy'] <= df['ParkingSpaceCount']\n",
    "\n",
    "parking_transactions = bc.sql('''\n",
    "    SELECT * \n",
    "    FROM parking_transactions \n",
    "    WHERE OccupancyDateTime >= DATE'2019-06-23'\n",
    "''')\n",
    "parking_transactions['time_prior_1h'] = calculate_time_deltas(parking_transactions, -1, 'h')\n",
    "parking_transactions['not_outlier'] = mark_outliers(parking_transactions.merge(parking_locations[['SourceElementKey', 'ParkingSpaceCount']]))\n",
    "\n",
    "parking_transactions = parking_transactions.query('not_outlier')\n",
    "bc.create_table('parking_transactions', parking_transactions)\n",
    "\n",
    "print(f'Number of transactions without outliers: {len(parking_transactions):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create a table with average spot occupancy at any given hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parking_transactions_agg = bc.sql('''\n",
    "    SELECT SourceElementKey\n",
    "        , transaction_year\n",
    "        , transaction_month\n",
    "        , transaction_day\n",
    "        , transaction_hour\n",
    "        , AVG(CAST(PaidOccupancy AS FLOAT)) AS average_occupancy\n",
    "    FROM (\n",
    "        SELECT A.*\n",
    "            , YEAR(OccupancyDateTime) AS transaction_year \n",
    "            , MONTH(OccupancyDateTime) AS transaction_month\n",
    "            , DAYOFMONTH(OccupancyDateTime) AS transaction_day\n",
    "            , HOUR(OccupancyDateTime) AS transaction_hour\n",
    "        FROM parking_transactions AS A\n",
    "    ) AS outer_query\n",
    "    GROUP BY SourceElementKey\n",
    "        , transaction_year\n",
    "        , transaction_month\n",
    "        , transaction_day\n",
    "        , transaction_hour\n",
    "    ''')\n",
    "\n",
    "bc.create_table('parking_transactions_agg', parking_transactions_agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can join the aggregates with the `parking_transactions` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_for_training = bc.sql('''\n",
    "    WITH temp_query_prior_1h AS (\n",
    "        SELECT A.SourceElementKey\n",
    "            , A.PaidOccupancy AS Label\n",
    "            , A.OccupancyDateTime\n",
    "            , B.average_occupancy as AvgOccupancy_prior_1h\n",
    "        FROM (\n",
    "            SELECT SourceElementKey\n",
    "                , PaidOccupancy\n",
    "                , OccupancyDateTime\n",
    "                , YEAR(time_prior_1h) AS transaction_year \n",
    "                , MONTH(time_prior_1h) AS transaction_month\n",
    "                , DAYOFMONTH(time_prior_1h) AS transaction_day\n",
    "                , HOUR(time_prior_1h) AS transaction_hour\n",
    "            FROM parking_transactions) AS A\n",
    "        LEFT OUTER JOIN parking_transactions_agg AS B\n",
    "            ON A.SourceElementKey = B.SourceElementKey\n",
    "                AND A.transaction_year = B.transaction_year\n",
    "                AND A.transaction_month = B.transaction_month\n",
    "                AND A.transaction_day = B.transaction_day\n",
    "                AND A.transaction_hour = B.transaction_hour\n",
    "    )\n",
    "    SELECT hr_1.* \n",
    "    FROM temp_query_prior_1h AS hr_1\n",
    "    \n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_for_training.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We no longer will need the `SourceElementKey` nor the `OccupancyDateTime` columns so let's drop them. Also, there may be some `N\\A`s so we can use `.dropna()` to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_for_training = (\n",
    "    dataset_for_training[[\n",
    "        'Label'\n",
    "        , 'AvgOccupancy_prior_1h'\n",
    "    ]]\n",
    "    .dropna()\n",
    ")\n",
    "dataset_for_training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CLEAN UP TO FREE UP SOME MEMORY\n",
    "bc.drop_table('parking_transactions')\n",
    "del parking_transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build regression model\n",
    "\n",
    "In this step we will build a Random Forest Regression model to predict the parking spot occupancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.preprocessing import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's split the dataset into 4 subsets: training-features, training-label, testing-features, and testing-label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_for_training['Label'] = dataset_for_training['Label'].astype('float32')\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(\n",
    "    dataset_for_training[['AvgOccupancy_prior_1h']]\n",
    "    , dataset_for_training['Label']\n",
    "    , train_size=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Full size: {len(dataset_for_training):,}, size of training: {len(train_y):,}, size of testing: {len(test_y):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to build our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml import RandomForestRegressor\n",
    "from cuml.metrics.regression import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr = RandomForestRegressor(\n",
    "    n_estimators=10\n",
    "    , max_depth=10\n",
    "    , min_rows_per_node=100\n",
    "    , verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'R-squered score for our model: {r2_score(test_y, rfr.predict(test_X))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a graph or roads in Seattle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In what follows we will build and traverse a graph of roads in Seattle.\n",
    "\n",
    "But first we need to extract the `Lat` and `Lon` from the string column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractLon(location):\n",
    "    lon = location.str.extract('([0-9\\.\\-]+) ([0-9\\.]+)')[0]\n",
    "    return lon\n",
    "\n",
    "def extractLat(location):\n",
    "    lon = location.str.extract('([0-9\\.\\-]+) ([0-9\\.]+)')[1]\n",
    "    return lon\n",
    "    \n",
    "parking_locations['longitude'] = extractLon(parking_locations['Location']).astype('float32')\n",
    "parking_locations['latitude'] = extractLat(parking_locations['Location']).astype('float32')\n",
    "\n",
    "parking_locations[['Location', 'longitude', 'latitude']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As crow flies vs as people walk\n",
    "\n",
    "Previously, our approach assumed we could *fly*. More to it, we could fly through buildings! However, we know that's *normally* not true. So, in what follows we will use a graph of road intersections and road lengths kindly donated by John Murray from Fusion Data Science in an attempt to provide a more realistic way of walking.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fff](https://miro.medium.com/max/1400/1*zmMox5tjq93AiBZdPRG3gg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's read the King County road data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "king_county_dir = 's3://bsql/data/seattle_parking/king_county_road_graph/'\n",
    "\n",
    "road_graph_data = cudf.read_csv(\n",
    "    f'{king_county_dir}king_county_road_graph_20190909.csv'\n",
    "    , storage_options={'anon': True}\n",
    ")\n",
    "road_graph_data['node1'] = road_graph_data['node1'].astype('int32')\n",
    "road_graph_data['node2'] = road_graph_data['node2'].astype('int32')\n",
    "road_graph_data['LENGTH'] = road_graph_data['LENGTH'] * 3 # convert to feet as the LENGHT was given in yards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "road_nodes = cudf.read_csv(\n",
    "    f'{king_county_dir}king_county_road_nodes_20190909.csv'\n",
    "    , storage_options={'anon': True}\n",
    ")\n",
    "road_nodes['NodeID'] = road_nodes['NodeID'].astype('int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the maximum of the `NodeID` so we can later append the additional nodes that will be perpendicular to the actual parking locations. We also specify the offset - this will be used to append parking nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 100000\n",
    "nodeId = road_nodes['NodeID'].max()                       ## so we can number the parking nodes properly (since we'll be adding a perpendicular projections)\n",
    "parking_nodes_idx = road_nodes['NodeID'].max() + offset   ## retain it so we can later filter the results to only parking locations\n",
    "nodeId"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move all the parking locations to host (via `.to_pandas()` method) so we can loop through all the ~1500 parking locations. Here, we also create an empty DataFrame that will hold the parking location nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = parking_locations.to_pandas().to_dict('records')\n",
    "parking_locations_nodes = cudf.DataFrame(columns=['NodeID', 'Lon', 'Lat', 'SourceElementKey'])\n",
    "added_location_edges    = cudf.DataFrame(columns=['node1', 'node2', 'LENGTH'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's process the parking data. The kernel below finds equations of two lines:\n",
    "\n",
    "1. Line that goes through road intersections\n",
    "2. Line that is perpendicular to (1) and goes through the parking location.\n",
    "\n",
    "![lll](https://miro.medium.com/max/1296/1*4Sg3alMbrT3DndIzM9dS4Q.gif)\n",
    "\n",
    "Ultimately, we are finind the intersection of these two lines -- we call it the `PROJ` point below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_find_projection(Lon_x, Lat_x, Lon_y, Lat_y, Lon_PROJ, Lat_PROJ, Lon_REF, Lat_REF):\n",
    "    for i, (lon_x, lat_x, lon_y, lat_y) in enumerate(zip(Lon_x, Lat_x, Lon_y, Lat_y)):\n",
    "        # special case where A and B have the same LON i.e. vertical line\n",
    "        if lon_x == lon_y:\n",
    "            Lon_PROJ[i] = lon_x\n",
    "            Lat_PROJ[i] = Lat_REF    \n",
    "        else:\n",
    "            # find slope\n",
    "            a_xy = (lat_x - lat_y) / float(lon_x - lon_y)\n",
    "\n",
    "            # special case where A and B have the same LAT i.e. horizontal line\n",
    "            if a_xy == 0:\n",
    "                Lon_PROJ[i] = Lon_REF\n",
    "                Lat_PROJ[i] = lat_x\n",
    "            else: \n",
    "                # if neither of the above special cases apply\n",
    "                # find the equation of the perpendicular line\n",
    "                a_R  = -1 / a_xy                    ### SLOPE\n",
    "\n",
    "                # find intersections\n",
    "                b_xy = lat_x - a_xy * lon_x\n",
    "                b_R  = Lat_REF - a_R  * Lon_REF\n",
    "\n",
    "                # find the coordinates\n",
    "                Lon_PROJ[i] = (b_R - b_xy) / (a_xy - a_R)\n",
    "                Lat_PROJ[i] = a_R * Lon_PROJ[i] + b_R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "parking_locations_cnt = len(locations)\n",
    "print('Number of parking locations: {0:,}'.format(parking_locations_cnt))\n",
    "\n",
    "for i, loc in enumerate(locations):\n",
    "    if i % 100 == 0:\n",
    "        print('Processed: {0:,} ({1:.2%}) nodes'.format(i, i/float(parking_locations_cnt)))\n",
    "        \n",
    "    #### INCREASE THE COUNTER AND GET THE REFERENCE POINT\n",
    "    nodeId = nodeId + 1\n",
    "    lat_r = loc['latitude']\n",
    "    lon_r = loc['longitude']\n",
    "\n",
    "    #### APPEND GEO COORDINATES TO INTERSECTION AND SUBSET DOWN THE DATASET\n",
    "    #### TO POINTS WITHIN ~2000ft FROM PARKING SPOT\n",
    "    paths = (\n",
    "        road_graph_data\n",
    "        .rename(columns={'node1': 'NodeID'})\n",
    "        .merge(road_nodes[['NodeID', 'Lat', 'Lon']], on='NodeID', how='left')\n",
    "        .rename(columns={'NodeID': 'node1', 'node2': 'NodeID'})\n",
    "        .merge(road_nodes[['NodeID', 'Lat', 'Lon']], on='NodeID', how='left')\n",
    "        .rename(columns={'NodeID': 'node2'})\n",
    "        .query('Lat_x >= (@lat_r - 0.005) and Lat_x <= (@lat_r + 0.005)')\n",
    "        .query('Lon_x >= (@lon_r - 0.005) and Lon_x <= (@lon_r + 0.005)')\n",
    "        .query('Lat_y >= (@lat_r - 0.005) and Lat_y <= (@lat_r + 0.005)')\n",
    "        .query('Lon_y >= (@lon_r - 0.005) and Lon_y <= (@lon_r + 0.005)')\n",
    "    )\n",
    "\n",
    "    #### APPEND THE PARKING LOCATION SO WE CAN CALCULATE DISTANCES\n",
    "    paths['Lon_REF'] = loc['longitude']\n",
    "    paths['Lat_REF'] = loc['latitude']\n",
    "\n",
    "    paths = paths.apply_rows(\n",
    "        kernel_find_projection\n",
    "        , incols  = ['Lon_x', 'Lat_x', 'Lon_y', 'Lat_y', 'Lon_REF', 'Lat_REF']\n",
    "        , outcols = {'Lon_PROJ': np.float64, 'Lat_PROJ': np.float64}\n",
    "        , kwargs  = {'Lon_REF': loc['longitude'], 'Lat_REF': loc['latitude']}\n",
    "    )\n",
    "\n",
    "    #### CALCULATE THE DISTANCES SO WE CAN CHECK IF THE PROJ POINT IS BETWEEN ROAD NODES\n",
    "    paths['Length_x_PROJ'] = cuspatial.haversine_distance(\n",
    "              paths['Lon_x']\n",
    "            , paths['Lat_x']\n",
    "            , paths['Lon_PROJ']\n",
    "            , paths['Lat_PROJ'])# * 0.621371 * 5280\n",
    "    paths['Length_x_PROJ'] = paths['Length_x_PROJ'] * 0.621371 * 5280\n",
    "\n",
    "    paths['Length_y_PROJ'] = cuspatial.haversine_distance(\n",
    "              paths['Lon_y']\n",
    "            , paths['Lat_y']\n",
    "            , paths['Lon_PROJ']\n",
    "            , paths['Lat_PROJ']) \n",
    "    paths['Length_y_PROJ'] = paths['Length_y_PROJ'] * 0.621371 * 5280\n",
    "\n",
    "    paths['Length_REF_PROJ'] = cuspatial.haversine_distance(\n",
    "              paths['Lon_REF']\n",
    "            , paths['Lat_REF']\n",
    "            , paths['Lon_PROJ']\n",
    "            , paths['Lat_PROJ']) \n",
    "    paths['Length_REF_PROJ'] = paths['Length_REF_PROJ'] * 0.621371 * 5280\n",
    "\n",
    "    #### SELECT THE POINTS THAT A LESS THAN OR EQAL TO TOTAL LENGTH OF THE EDGE (WITHIN 1 ft)\n",
    "    paths['PROJ_between'] = (paths['Length_x_PROJ'] + paths['Length_y_PROJ']) <= (paths['LENGTH'] + 1)\n",
    "    \n",
    "    #### SELECT THE CLOSEST\n",
    "    closest = (\n",
    "        paths\n",
    "        .query('PROJ_between')\n",
    "        .nsmallest(1, 'Length_REF_PROJ')\n",
    "        .to_pandas()\n",
    "        .to_dict('records')[0]\n",
    "    )\n",
    "\n",
    "    # add nodes\n",
    "    nodes =    cudf.DataFrame({\n",
    "          'NodeID': [nodeId + offset, nodeId]\n",
    "        , 'Lon':    [closest['Lon_REF'], closest['Lon_PROJ']]\n",
    "        , 'Lat':    [closest['Lat_REF'], closest['Lat_PROJ']]\n",
    "        , 'SourceElementKey': [loc['SourceElementKey'], None]\n",
    "    })\n",
    "\n",
    "    parking_locations_nodes = cudf.concat([parking_locations_nodes, nodes])\n",
    "\n",
    "    # add edges (bi-directional)\n",
    "    edges = cudf.DataFrame({\n",
    "          'node1':  [nodeId, nodeId, nodeId, closest['node1'], closest['node2'], nodeId + offset]\n",
    "        , 'node2':  [closest['node1'], closest['node2'], nodeId + offset, nodeId, nodeId, nodeId]\n",
    "        , 'LENGTH': [\n",
    "              closest['Length_x_PROJ'], closest['Length_y_PROJ'], closest['Length_REF_PROJ']\n",
    "            , closest['Length_x_PROJ'], closest['Length_y_PROJ'], closest['Length_REF_PROJ']\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    added_location_edges = cudf.concat([added_location_edges, edges]) ## append to the temp DataFrame\n",
    "\n",
    "print('Finished processing...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parking_locations_nodes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "road_graph_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "road_nodes = (\n",
    "    cudf\n",
    "    .concat([road_nodes[['NodeID', 'Lon', 'Lat']], parking_locations_nodes])\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can find the nearest intersections from the Space Needle!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = {'latitude': 47.620422, 'longitude': -122.349358}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "road_nodes['Lon_REF'] = location['longitude']\n",
    "road_nodes['Lat_REF'] = location['latitude']\n",
    "\n",
    "road_nodes['Distance'] = cuspatial.haversine_distance(\n",
    "          road_nodes['Lon']\n",
    "        , road_nodes['Lat']\n",
    "        , road_nodes['Lon_REF']\n",
    "        , road_nodes['Lat_REF']) \n",
    "road_nodes['Distance'] = road_nodes['Distance'] * 0.621371 * 5280\n",
    "\n",
    "space_needle_to_nearest_intersection = road_nodes.nsmallest(5, 'Distance') ### Space Needle is surrounded by around 5 road intersections hence we add 5\n",
    "space_needle_to_nearest_intersection_dist = space_needle_to_nearest_intersection['Distance'].to_array()[0]\n",
    "\n",
    "space_needle_to_nearest_intersection['node1'] = nodeId + 2\n",
    "space_needle_to_nearest_intersection = (\n",
    "    space_needle_to_nearest_intersection\n",
    "    .rename(columns={'NodeID': 'node2', 'Distance': 'LENGTH'})\n",
    "    [['node1', 'node2', 'LENGTH']]\n",
    ")\n",
    "\n",
    "road_graph_data = cudf.concat([space_needle_to_nearest_intersection, added_location_edges, road_graph_data])\n",
    "space_needle_to_nearest_intersection ### SHOW THE EDGES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The road graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "road_graph_data = road_graph_data.reset_index(drop=True)\n",
    "road_graph_data['node1'] = road_graph_data['node1'].astype('int32')\n",
    "road_graph_data['node2'] = road_graph_data['node2'].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = cugraph.Graph()\n",
    "g.from_cudf_edgelist(\n",
    "    road_graph_data\n",
    "    , source='node1'\n",
    "    , destination='node2'\n",
    "    , edge_attr='LENGTH'\n",
    "    , renumber=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the `.sssp(...)` method from `cugraph` to find the shortest distances to parking spots from the Space Needle!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_distances = cugraph.sssp(g, nodeId + 2)\n",
    "distances = all_distances.query('vertex > @parking_nodes_idx and distance < 1000')\n",
    "distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the regression model\n",
    "\n",
    "To use our regression model, let's retrieve the `1hr` feature for all the parking spots from the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = dttm.datetime.strptime('2019-06-24 13:21:00', '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "inference = distances.merge(parking_locations_nodes, left_on='vertex', right_on='NodeID')\n",
    "inference['OccupancyDateTime'] = date\n",
    "inference['time_prior_1h'] = date + dttm.timedelta(hours=-1)\n",
    "bc.create_table('inference', inference)\n",
    "inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference = bc.sql(f'''\n",
    "    WITH temp_query_prior_1h AS (\n",
    "        SELECT A.SourceElementKey\n",
    "            , A.OccupancyDateTime\n",
    "            , B.average_occupancy as AvgOccupancy_prior_1h\n",
    "        FROM (\n",
    "            SELECT SourceElementKey\n",
    "                , OccupancyDateTime\n",
    "                , YEAR(time_prior_1h) AS transaction_year \n",
    "                , MONTH(time_prior_1h) AS transaction_month\n",
    "                , DAYOFMONTH(time_prior_1h) AS transaction_day\n",
    "                , HOUR(time_prior_1h) AS transaction_hour\n",
    "            FROM inference) AS A\n",
    "        LEFT OUTER JOIN parking_transactions_agg AS B\n",
    "            ON A.SourceElementKey = B.SourceElementKey\n",
    "                AND A.transaction_year = B.transaction_year\n",
    "                AND A.transaction_month = B.transaction_month\n",
    "                AND A.transaction_day = B.transaction_day\n",
    "                AND A.transaction_hour = B.transaction_hour\n",
    "    )\n",
    "    SELECT hr_1.* \n",
    "    FROM temp_query_prior_1h AS hr_1\n",
    "    ORDER BY SourceElementKey\n",
    "    \n",
    "''')\n",
    "inference.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_X = (\n",
    "    inference[[\n",
    "        'SourceElementKey'\n",
    "        , 'AvgOccupancy_prior_1h'\n",
    "    ]]\n",
    "    .merge(parking_locations[['SourceElementKey']], on=['SourceElementKey'])\n",
    "    .sort_values(by='SourceElementKey')\n",
    "    .dropna()\n",
    "    .drop(columns=['SourceElementKey'])\n",
    ")\n",
    "inference_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference.join(rfr.predict(inference_X).to_frame('prediction'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stepping through the graph\n",
    "\n",
    "`cugraph` returns a DataFrame with vertex, distance to that vertex, and the total distance traveled to that vertex from the `nodeId + 1` node -- the Space Needle. Here, we unfold the full path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfold -- create the whole path\n",
    "closest_node = nodeId + 2\n",
    "parking_cnt = distances['vertex'].count()\n",
    "\n",
    "for i in range(parking_cnt):\n",
    "    print('Processing record: {0}'.format(i))\n",
    "    parking_node = distances.iloc[i]\n",
    "\n",
    "    vertex = int(parking_node['vertex'])\n",
    "    predecessor = int(parking_node['predecessor'])\n",
    "    \n",
    "    if i == 0:\n",
    "        paths = all_distances.query('vertex == @vertex')\n",
    "    else:\n",
    "        paths = cudf.concat([all_distances.query('vertex == @vertex'), paths])\n",
    "\n",
    "    while vertex != closest_node:\n",
    "        temp = all_distances.query('vertex == @predecessor')\n",
    "        paths = cudf.concat([temp, paths])\n",
    "        predecessor = temp['predecessor'].to_array()[0]\n",
    "        vertex = temp['vertex'].to_array()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you were to plot these, this is roughtly what you would get\n",
    "![map](https://miro.medium.com/max/1400/1*UeOfPlEjzyRt-gxUHauUIw.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAPIDS Stable",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
