{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mining COVID-19 Kaggle competition scientific papers to build an understanding of viruses\n",
    "## Part 2. Processing and featurizing data\n",
    "\n",
    "Working off of a clean metadata file, in this notebook we will featurize the subset of the JSON files that we downloaded from AI2 S3 repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import cupy\n",
    "import s3fs\n",
    "\n",
    "pd.options.display.max_rows = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and process the JSON files\n",
    "All the data is located in our S3 bucket: `s3://bsql/data/covid`. However, the metadata file we saved in the root of this folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 's3://bsql/data/covid'\n",
    "fs = s3fs.S3FileSystem(anon=True)\n",
    "metadata_clean = cudf.read_csv('metadata_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process below is lengthy. The issue here is the format of the JSON files that requires us to loop through this one-by-one, read the contents using the `json` package, and only then extract the interesting information. You can adapt how much time you can spend or copy the data locally to process; we default to 300 since this should not take more than 15-30 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "articles_list = []\n",
    "batch_size = 300\n",
    "\n",
    "read_subset = True\n",
    "paper_cnt = batch_size if read_subset else len(metadata_clean)\n",
    "\n",
    "for i in range(0, paper_cnt, batch_size):\n",
    "    print(f'Processing articles {i}:{i+batch_size}')\n",
    "    files = [f'{data_dir}/{f}' for f in metadata_clean.iloc[i:i+batch_size,:]['pdf_json_files'].to_array()]\n",
    "\n",
    "    papers = []\n",
    "\n",
    "    for f in files:\n",
    "        with fs.open(f, 'r') as ff:\n",
    "            json_read = json.loads(ff.read())\n",
    "\n",
    "        for i, s in enumerate(json_read['body_text']):\n",
    "                papers.append((\n",
    "                    json_read['paper_id']  #### key: SHA\n",
    "                    , s['section']         #### section title\n",
    "                    , i                    #### paragraph\n",
    "                    , s['text']            #### text of the paragraph\n",
    "                ))\n",
    "\n",
    "    articles_list.append(\n",
    "            cudf.DataFrame(\n",
    "                papers\n",
    "                , columns=['sha', 'section', 'paragraph', 'text']\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    del papers\n",
    "    del files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can concatenate all the small cudf DataFrames into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = (\n",
    "    cudf.concat(\n",
    "        articles_list#[1:]\n",
    "    ).reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'We read {len(articles):,} paragraphs.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data featurization\n",
    "\n",
    "First step on the way to featurize our dataset - we need to create a vocabulary file. The vocabulary needs to conform to the same format as it is expected by the BERT models. \n",
    "\n",
    "## Build vocabulary\n",
    "\n",
    "In the first step we will simply tokenize the strings into words, normalize the strings to lower, and remove some of the punctuation signs we don't need. The `tokenize()` method splits a string on a space and puts every tokenized word in a `cudf.Series`. Next, we aggregate and count the occurence of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_articles(frame, col):\n",
    "    temp = frame[col].str.tokenize().to_frame()\n",
    "    temp['text'] = temp['text'].str.lower()\n",
    "    temp['text'] = temp['text'].str.replace('[\\.?,#\"$!;:=\\(\\)\\-\\+0-9]', '')\n",
    "    temp['counter'] = 1\n",
    "    return temp\n",
    "\n",
    "min_count = 50\n",
    "\n",
    "token_counts = (\n",
    "    tokenize_articles(articles, 'text')\n",
    "    .groupby('text')\n",
    "    .count()\n",
    "    .reset_index()\n",
    "    .sort_values(by='counter', ascending=False)\n",
    "    .query(f'counter > {min_count}')\n",
    ")\n",
    "\n",
    "token_counts = token_counts.to_pandas()\n",
    "\n",
    "print(f'Total number of tokens: {len(token_counts)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look what this looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't want the space so let's remove the record with index `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts = token_counts.loc[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the final vocabulary we will be using a `SubwordTextEncoder` from this repository: https://github.com/kwonmha/bert-vocab-builder/. The script we use is further slightly modified to remove the dependency on Tensorflow.\n",
    "\n",
    "The algorithm scans the words and iteratively builds a vocabulary of the longest subwords that the original words can be subdivided into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts import text_encoder\n",
    "\n",
    "sw = text_encoder.SubwordTextEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SubwordTextEncoder` expects a dictionary with keys being the words and the values being the word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts_dict = dict(token_counts.to_dict('split')['data'])\n",
    "\n",
    "sw.build_from_token_counts(\n",
    "      token_counts_dict\n",
    "      , 50\n",
    "      , 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look what we got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = (\n",
    "    cudf.Series(sw._all_subtoken_strings)\n",
    "    .sort_values()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "with open('vocabulary.txt', 'w') as f:\n",
    "    f.writelines([f'{item}\\n' for item in list(vocab.to_array())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the hash version of the vocabulary\n",
    "The `subword_tokenizer` requires an encoded version of the vocabulary to tokenize to the representation BERT is expecting. The script from CLX achieves that: https://github.com/rapidsai/clx/blob/80d3198dfe54bef704d177404873d2312a77f2c9/python/clx/analytics/perfect_hash.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts import perfect_hash\n",
    "\n",
    "perfect_hash.hash_vocab(\n",
    "    'vocabulary.txt'\n",
    "    , 'vocabulary_hash.txt'\n",
    "    , False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize text\n",
    "Now we are ready to tokenize the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subword_tokenize(frame):\n",
    "    num_strings = len(frame.text)\n",
    "    num_bytes = frame.text.str.byte_count().sum()\n",
    "\n",
    "    tokens, attention = frame.text.str.lower().str.subword_tokenize(\n",
    "        'vocabulary_hash.txt'          #### hashed vocabulary file\n",
    "        , 256                          #### maximum length of a sequence\n",
    "        , 256                          #### stride\n",
    "        , max_num_strings=num_strings  #### maximum number of strings to return\n",
    "        , max_num_chars=num_bytes      #### maximum number of characters\n",
    "        , max_rows_tensor=num_strings  #### maximum number of rows\n",
    "        , do_lower=True                #### if True the original text will be lower-cased before encoding\n",
    "        , do_truncate=True             #### if True the strings will be truncated or padded to the maximum length\n",
    "    )[:2]\n",
    "    \n",
    "    temp = cudf.DataFrame()\n",
    "    temp['tokens'] = tokens\n",
    "    temp['attention'] = attention\n",
    "    \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = subword_tokenize(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how many tokens we get from the 300 articles we read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_cnt = len(tokenized)\n",
    "articles_cnt = len(articles)\n",
    "\n",
    "print(f'There are {tokens_cnt:,} tokens in the dataset.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each token has a maximum (padded) length of 256, if we divide the total number of tokens by 256 we should get the total number of paragraphs in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tokens_cnt / 256 == articles_cnt\n",
    "print(f'Number of paragraphs derived from tokens: {int(tokens_cnt / 256):,}, actual number of paragraphs: {articles_cnt:,}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
