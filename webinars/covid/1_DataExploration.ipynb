{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mining COVID-19 Kaggle competition scientific papers to build an understanding of viruses\n",
    "## Part 1. Exploring the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coronaviruses have been around for decades but few to none such deadly and easily spreading as the COVID-19 thus far. Earlier this year, Allen Institute for AI (AI2) and a consortium of research institutes along with the White House curated a corpus of scientific papers on coronaviruses published since 19th century and offered a Kaggle competition to analyze it and answer some questions about different aspects of the virus, like how it spreads, or how it affects living organisms.\n",
    "\n",
    "In this notebook we will explore and clean up the dataset published as part of this competition. The dataset is maintained by AI2 and refreshed daily; you can download the newest here: [https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases.html](https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases.html). In this installment we will use data from 2020-08-24 and that I have already uploaded to our S3 bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "\n",
    "Let's start with imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import pandas as pd\n",
    "import json\n",
    "import s3fs\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data\n",
    "Let's read the data. The metadata table contains a list of all articles released as part of this corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 's3://bsql/data/covid'\n",
    "\n",
    "metadata = cudf.read_csv(f'{data_dir}/metadata.csv', storage_options={'anon': True})\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the list of all the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the columns are IDs like cord_uid, or doi among others. However, of the more interesting to us, we will focus on\n",
    "\n",
    "* **title**, **abstract** and **authors** we will use to see if we have any duplicates\n",
    "* **publish_time** that shows when the article was published\n",
    "* **source_x** which shows the source where the article was originated from\n",
    "* the **journal** column list the journal which the article was published in\n",
    "* **pdf_json_files** that shows the location of the file with the body text of the paper; we will use this to see if we have any missing files so we can exclude them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blazing SQL\n",
    "We will interchangeably use Blazing SQL throughout the notebook so let's start the `BlazingContext` and create the `metadata` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blazingsql import BlazingContext\n",
    "\n",
    "# Start up BlazingSQL\n",
    "bc = BlazingContext()\n",
    "\n",
    "# Create table from CSV\n",
    "bc.create_table('metadata', metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration\n",
    "Let's get familiar with the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Article count\n",
    "I normally start by counting the number of rows I have in any dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Total number of articles: {len(metadata):,}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source\n",
    "Next, let's explore the sources i.e. the repositories of articles where these are hosted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bc.sql('''\n",
    "    SELECT source_x\n",
    "        , COUNT(*) AS cnt \n",
    "    FROM metadata \n",
    "    GROUP BY source_x \n",
    "    ORDER BY cnt DESC \n",
    "    LIMIT 10\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Journals\n",
    "Next is journals -- let's check the list of journals these papers were published in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.groupby(by='journal').agg({'cord_uid': 'count'}).sort_values(by='cord_uid', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publication years\n",
    "Around 2003 there was a big spike in terms of number of papers published on coronaviruses that is correlated with #SARS-Cov-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata['year_published'] = metadata.publish_time.str.extract('([0-9\\.\\-]{4})')[0].astype('int16')\n",
    "bc.create_table('metadata', metadata)\n",
    "\n",
    "(\n",
    "    metadata\n",
    "    .query('year_published < 2020')\n",
    "    .groupby(by='year_published')\n",
    "    .agg({'source_x': 'count'})\n",
    "    .to_pandas()\n",
    "    .plot(kind='bar', figsize=(18,9))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially we filtered out the results that included year 2020. Here's why..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    metadata\n",
    "    .groupby(by='year_published')\n",
    "    .agg({'source_x': 'count'})\n",
    "    .to_pandas()\n",
    "    .plot(kind='bar', figsize=(18,9))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleanup\n",
    "\n",
    "It is now time for us to have a look at the data itself. What I like to do first to any dataset is to check for the missing observations.\n",
    "\n",
    "## Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(metadata.columns)[2:-3]\n",
    "\n",
    "query_missing = 'SELECT '\n",
    "query_missing += '\\n    ,'.join([f'CASE WHEN {col} IS NULL THEN 1 ELSE 0 END AS {col}_miss' for col in cols])\n",
    "query_missing += '\\nFROM metadata'\n",
    "\n",
    "query_unions = (\n",
    "    '\\nUNION ALL \\n    '\n",
    "    .join([\n",
    "        f\"SELECT '{col}_miss' AS miss_flag, \"\n",
    "        f\"{col}_miss AS FLAG, COUNT(*) AS CNT \"\n",
    "        f\"FROM missing_flags GROUP BY {col}_miss\" \n",
    "        for col in cols\n",
    "    ])\n",
    ")\n",
    "\n",
    "bc.create_table('missing_flags', bc.sql(query_missing))\n",
    "bc.create_table('missing_summary', bc.sql(query_unions))\n",
    "\n",
    "row_cnt = float(len(metadata))\n",
    "\n",
    "bc.sql(f'''\n",
    "    SELECT *\n",
    "        , CNT / {row_cnt} * 100.0 AS MISS_PCNT\n",
    "    FROM missing_summary \n",
    "    WHERE FLAG = 1\n",
    "    ORDER BY MISS_PCNT DESC\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the queries we use to generate the `_miss` flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(query_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query to create the aggregates is here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(query_unions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're missing almost 60% of values in the pdf_json_files, let's remove observations that are missing a value in this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc.create_table('metadata_no_missing', bc.sql('SELECT * FROM metadata WHERE pdf_json_files IS NOT NULL'))\n",
    "bc.sql('SELECT COUNT(*) FROM metadata_no_missing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rerun our query on the dataset with removed missing observations to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_missing = 'SELECT '\n",
    "query_missing += '\\n    ,'.join([f'CASE WHEN {col} IS NULL THEN 1 ELSE 0 END AS {col}_miss' for col in cols])\n",
    "query_missing += '\\nFROM metadata_no_missing'\n",
    "\n",
    "query_unions = (\n",
    "    '\\nUNION ALL \\n    '\n",
    "    .join([\n",
    "        f\"SELECT '{col}_miss' AS miss_flag, \"\n",
    "        f\"{col}_miss AS FLAG, COUNT(*) AS CNT \"\n",
    "        f\"FROM missing_flags GROUP BY {col}_miss\" \n",
    "        for col in cols\n",
    "    ])\n",
    ")\n",
    "\n",
    "bc.create_table('missing_flags', bc.sql(query_missing))\n",
    "bc.create_table('missing_summary', bc.sql(query_unions))\n",
    "\n",
    "row_cnt = float(len(metadata))\n",
    "\n",
    "bc.sql(f'''\n",
    "    SELECT *\n",
    "        , CNT / {row_cnt} * 100.0 AS MISS_PCNT\n",
    "    FROM missing_summary \n",
    "    WHERE FLAG = 1\n",
    "    ORDER BY MISS_PCNT DESC\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we can continue with this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duplicates\n",
    "It's now a good time to check if we have any duplicates in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_check_for_dupes = ['url', 'title', 'doi', 'abstract', 'pdf_json_files', 'pmc_json_files']\n",
    "\n",
    "col = cols_to_check_for_dupes[5]\n",
    "\n",
    "select_pattern = '{c}_DIST, {c}_MISS, {c}_DIST + {c}_MISS AS {c}_TTL, CNT - {c}_DIST - {c}_MISS AS {c}_DUPES'\n",
    "count_pattern  = 'COUNT(DISTINCT {c}) AS {c}_DIST, SUM(CASE WHEN {c} IS NULL THEN 1 ELSE 0 END) AS {c}_MISS'\n",
    "\n",
    "query = f'SELECT CNT\\n    , '\n",
    "query += '\\n    , '.join([select_pattern.format(c=c) for c in cols_to_check_for_dupes])\n",
    "query += '\\nFROM (\\n    SELECT COUNT(*) AS CNT, '\n",
    "query += '\\n        ,'.join([count_pattern.format(c=c) for c in cols_to_check_for_dupes])\n",
    "query += '\\n    FROM metadata_no_missing\\n) AS A'\n",
    "\n",
    "bc.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc.create_table('duplicates', bc.sql(query))\n",
    "\n",
    "query_dupes = 'SELECT '\n",
    "query_dupes += ','.join(f'{c}_DUPES' for c in cols_to_check_for_dupes)\n",
    "query_dupes += ' FROM duplicates'\n",
    "bc.sql(query_dupes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links to pdf JSON files\n",
    "URLs have no dupes and the other one are small. So, for the `pdf_json_files` let's see what rows these are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc.sql('SELECT pdf_json_files FROM metadata_no_missing GROUP BY pdf_json_files HAVING COUNT(*) > 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like one of the links is replicated 3 times. Let's check if the other fields are duplicates as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc.sql('''\n",
    "    SELECT A.*\n",
    "    FROM metadata_no_missing AS A\n",
    "    INNER JOIN (\n",
    "        SELECT pdf_json_files FROM metadata_no_missing GROUP BY pdf_json_files HAVING COUNT(*) > 1\n",
    "    ) AS B\n",
    "    ON A.pdf_json_files = B.pdf_json_files\n",
    "    ORDER BY A.pdf_json_files\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 3 articles seem simply mislabled as the pmc_json_files point to different xml files and they seem to be 3 different articles. The remaining seem to be duplicated entries. Since the number is low we remove them all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_json_clean = bc.sql('''\n",
    "    SELECT A.*\n",
    "    FROM metadata_no_missing AS A\n",
    "    INNER JOIN (\n",
    "        SELECT pdf_json_files FROM metadata_no_missing GROUP BY pdf_json_files HAVING COUNT(*) = 1\n",
    "    ) AS B\n",
    "    ON A.pdf_json_files = B.pdf_json_files\n",
    "    ORDER BY A.pdf_json_files\n",
    "''')\n",
    "                             \n",
    "bc.create_table('metadata_json_clean', metadata_json_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title duplicates\n",
    "We do have couple of title duplicates. Let's check them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc.sql('''SELECT A.title\n",
    "        , A.doi\n",
    "        , A.abstract\n",
    "        , A.authors\n",
    "        , A.journal\n",
    "        , A.year_published\n",
    "    FROM metadata_json_clean AS A\n",
    "    INNER JOIN (\n",
    "        SELECT title FROM metadata_json_clean GROUP BY title HAVING COUNT(*) > 1\n",
    "    ) AS B\n",
    "    ON A.title = B.title\n",
    "    ORDER BY A.title\n",
    "    LIMIT 10''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that these are truly duplicated records: somehow they differ with doi identifier but the titles, abstracts and authors mostly match. However, since there are over 1200 of duplicated records we would not like to drop all of these and will use the `.drop_duplicates()` method from to retain one record from each duplicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_title_clean = metadata_json_clean.drop_duplicates(subset=['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should have no duplicated titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc.create_table('metadata_title_clean', metadata_title_clean)\n",
    "bc.sql('SELECT title FROM metadata_title_clean GROUP BY title HAVING COUNT(*) > 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing JSON files\n",
    "Final check -- are there any files in the folder that are not listed in the metadata file, and vice versa?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_json = f'{data_dir}/document_parses/pdf_json'\n",
    "\n",
    "fs = s3fs.S3FileSystem(anon=True)\n",
    "files = ['/'.join(e.split('/')[-3:]) for e in fs.ls(pdf_json)]\n",
    "\n",
    "bc.create_table('files', cudf.DataFrame(files, columns=['pdf_json_files']))\n",
    "\n",
    "bc.create_table('pdf_files_meta', bc.sql('''\n",
    "    SELECT A.pdf_json_files AS meta_pdf_json\n",
    "        , B.pdf_json_files AS folder_pdf_json\n",
    "        , CASE WHEN A.pdf_json_files IS NULL THEN 1 ELSE 0 END meta_pdf_json_missing\n",
    "        , CASE WHEN B.pdf_json_files IS NULL THEN 1 ELSE 0 END folder_pdf_json_missing\n",
    "    FROM metadata_title_clean AS A\n",
    "    FULL OUTER JOIN files AS B\n",
    "        ON A.pdf_json_files = B.pdf_json_files\n",
    "'''))\n",
    "\n",
    "bc.sql('''\n",
    "    SELECT meta_pdf_json_missing\n",
    "        , folder_pdf_json_missing\n",
    "        , COUNT(*) AS CNT\n",
    "    FROM pdf_files_meta\n",
    "    GROUP BY meta_pdf_json_missing\n",
    "        , folder_pdf_json_missing\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, actually quite surprisingly, we have 12.5k files that we have links to in the metadata.csv file but cannot be found on disk, and 5k files that are present on disk but cannot be referenced in the metadata.csv file. Well, in this case, I decided to drop all the missing files thus keeping only the 87,438 files I can find in both, the metadata.csv file, and on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc.create_table('metadata_pdf_json_clean', bc.sql('''\n",
    "    SELECT A.*\n",
    "    FROM metadata_title_clean AS A\n",
    "    INNER JOIN (\n",
    "        SELECT meta_pdf_json\n",
    "        FROM pdf_files_meta\n",
    "        WHERE meta_pdf_json_missing = 0\n",
    "            AND folder_pdf_json_missing = 0\n",
    "    ) AS B\n",
    "        ON A.pdf_json_files = B.meta_pdf_json\n",
    "'''))\n",
    "\n",
    "bc.sql('SELECT COUNT(*) AS CNT FROM metadata_pdf_json_clean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save clean metadatafile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_metadata = bc.sql('SELECT * FROM metadata_pdf_json_clean')\n",
    "clean_metadata.to_csv(f'metadata_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
